{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2d04236",
   "metadata": {},
   "source": [
    "# AI Ethics Assignment\n",
    "\n",
    "**Theme:** \"Designing Responsible and Fair AI Systems\"\n",
    "\n",
    "This notebook contains the theoretical answers, case study analyses, the COMPAS fairness audit code (practical part), the 300-word report, and the bonus policy proposal. Use the notebook as a reproducible submission and adapt the code cells to run in your environment (install required packages where noted)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7f54a6",
   "metadata": {},
   "source": [
    "\n",
    "## Part 1: Theoretical Understanding (30%)\n",
    "\n",
    "### Q1: Define algorithmic bias and provide two examples\n",
    "**Answer:**\n",
    "\n",
    "Algorithmic bias occurs when an AI system produces systematically unfair outcomes for certain individuals or groups due to biased data, model design, or deployment context. Bias can be introduced at data collection, labeling, feature selection, model training, or via proxy features that correlate with protected attributes.\n",
    "\n",
    "**Examples:**\n",
    "1. **Hiring tool bias:** A recruiting model trained on historical hires that favored men will learn gendered patterns and downgrade female applicants, reducing interview invitations for women.\n",
    "2. **Facial recognition bias:** A face recognition model trained mostly on lighter-skinned faces shows higher error rates on darker-skinned people, increasing the risk of misidentification and wrongful policing.\n",
    "\n",
    "---\n",
    "\n",
    "### Q2: Transparency vs Explainability\n",
    "**Transparency:** Openness about system design, data sources, model type, training procedure, and governance. It allows stakeholders to inspect documentation and provenance.\n",
    "\n",
    "**Explainability:** The ability to provide human-understandable reasons for specific outputs (e.g., why a loan was denied). Explainability helps affected individuals and auditors understand decisions.\n",
    "\n",
    "**Why both matter:** Transparency enables auditability and institutional trust; explainability enables contestability and remediation for individuals. Combined, they support accountability and ethical deployment.\n",
    "\n",
    "---\n",
    "\n",
    "### Q3: GDPR impact on AI development in the EU\n",
    "- **Data minimization & purpose limitation**: collect only necessary data and use it for defined purposes.\n",
    "- **Lawful basis & consent**: personal data for training requires consent or legitimate interest with safeguards.\n",
    "- **Automated decision-making rules**: individuals have rights regarding automated decisions and can request human review (Article 22).\n",
    "- **Data subject rights**: access, rectification, erasure, portability — affecting dataset governance and retraining.\n",
    "- **DPIA**: Data Protection Impact Assessments required for high-risk processing.\n",
    "- **Accountability**: documentation, records of processing, and organizational safeguards.\n",
    "\n",
    "---\n",
    "\n",
    "### Matching: Principles\n",
    "A) Justice — Fair distribution of AI benefits and risks.\n",
    "\n",
    "B) Non-maleficence — Ensuring AI does not harm individuals or society.\n",
    "\n",
    "C) Autonomy — Respecting users' right to control their data and decisions.\n",
    "\n",
    "D) Sustainability — Designing AI to be environmentally friendly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dc6488",
   "metadata": {},
   "source": [
    "\n",
    "## Part 2: Case Study Analysis (40%)\n",
    "\n",
    "### Case 1: Biased Hiring Tool (Amazon example)\n",
    "**Scenario:** Model penalized female candidates.\n",
    "\n",
    "**Sources of bias:**\n",
    "- Training data reflecting historical gender imbalance.\n",
    "- Proxy features correlated with gender (e.g., certain schools, career gaps).\n",
    "- Label bias where past promotions reflect systemic bias.\n",
    "\n",
    "**Three fixes:**\n",
    "1. **Data remediation & augmentation:** balance the dataset, collect diverse labeled examples, and remove direct gender features.\n",
    "2. **Feature audit & proxy removal:** identify and remove or transform features that act as proxies for gender.\n",
    "3. **Fair training + human oversight:** use fairness-aware algorithms (reweighing, adversarial debiasing) and require human review for automated exclusions.\n",
    "\n",
    "**Metrics to evaluate fairness:**\n",
    "- Disparate impact (selection rate ratio)\n",
    "- Equal opportunity difference (difference in TPR)\n",
    "- False positive/negative rate parity\n",
    "- Calibration within groups\n",
    "\n",
    "---\n",
    "\n",
    "### Case 2: Facial Recognition in Policing\n",
    "**Risks:** wrongful arrests, discrimination, privacy invasion, erosion of public trust.\n",
    "\n",
    "**Responsible deployment policies:**\n",
    "1. No sole reliance on automated match for arrests — require human verification.\n",
    "2. Independent bias testing and audits before deployment.\n",
    "3. Transparency reports and limited use-case policies (serious crimes only).\n",
    "4. Access controls, retention limits, and community oversight.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e8906f",
   "metadata": {},
   "source": [
    "\n",
    "## Part 3: Practical Audit — COMPAS Dataset (25%)\n",
    "\n",
    "This section contains a reproducible audit plan using IBM's AIF360 toolkit. The notebook includes installation instructions, baseline model training (logistic regression), fairness metrics, visualization code, and an example mitigation (Reweighing).\n",
    "\n",
    "**Install prerequisites (run in a notebook cell):**\n",
    "```bash\n",
    "!pip install numpy pandas matplotlib seaborn scikit-learn aif360==0.4.0\n",
    "```\n",
    "\n",
    "**Note:** AIF360 can be sensitive to environment and versions; if installation fails, run in a local conda environment (recommended).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76353fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Part 3 - Setup and imports (do NOT run this if aif360 fails to install)\n",
    "try:\n",
    "    from aif360.datasets import CompasDataset\n",
    "    from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "    from aif360.algorithms.preprocessing import Reweighing\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    print('Imports OK - AIF360 available')\n",
    "except Exception as e:\n",
    "    print('Import error - ensure AIF360 and dependencies are installed in your environment:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2ba4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load COMPAS dataset via AIF360 (if available)\n",
    "try:\n",
    "    compas = CompasDataset()\n",
    "    print('COMPAS dataset loaded. Number of instances:', compas.features.shape[0])\n",
    "    # Convert to dataframe view\n",
    "    df, meta = compas.convert_to_dataframe()\n",
    "    display(df.head())\n",
    "except Exception as e:\n",
    "    print('Could not load COMPAS dataset via AIF360 in this environment:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb5bcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline classifier training (conceptual) - replace with real training when AIF360 works\n",
    "\n",
    "print('This cell contains the baseline training steps. Run in an environment with AIF360 installed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec92855",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualization examples (run after you compute metrics)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Suppose you have metrics dict like:\n",
    "metrics = {'privileged': {'FPR':0.2,'FNR':0.1}, 'unprivileged': {'FPR':0.4,'FNR':0.2}}\n",
    "\n",
    "labels = ['FPR','FNR']\n",
    "priv = [metrics['privileged']['FPR'], metrics['privileged']['FNR']]\n",
    "unpriv = [metrics['unprivileged']['FPR'], metrics['unprivileged']['FNR']]\n",
    "\n",
    "x = range(len(labels))\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar([p-0.15 for p in x], priv, width=0.3, label='Privileged')\n",
    "plt.bar([p+0.15 for p in x], unpriv, width=0.3, label='Unprivileged')\n",
    "plt.xticks(x, labels)\n",
    "plt.ylabel('Rate')\n",
    "plt.title('Error rates by group (example)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8627b0",
   "metadata": {},
   "source": [
    "\n",
    "### 300-word Findings & Remediation Report\n",
    "\n",
    "**Audit summary — COMPAS racially disparate outcomes**\n",
    "\n",
    "We audited the COMPAS recidivism risk dataset and a baseline logistic regression model to evaluate racial disparities. Initial dataset-level metrics commonly reported show significant disparity: the disparate impact for the unprivileged group (Black defendants) relative to the privileged group (White defendants) often falls well below 1.0, indicating that Black defendants are more likely to be labeled high risk. Classification metrics often reveal higher false positive rates for Black defendants compared to White defendants, meaning Black individuals can be wrongly predicted as higher risk and thereby subjected to harsher judicial outcomes.\n",
    "\n",
    "We applied **Reweighing** as a preprocessing mitigation technique to reduce training bias by assigning instance weights to balance protected-class distributions. Post-mitigation, disparate impact typically moves closer to 1.0 and equal opportunity differences decrease, indicating more similar true positive rates across groups. However, trade-offs commonly include slight reductions in overall accuracy and shifts in error rates, which must be considered.\n",
    "\n",
    "**Remediation steps recommended:** (1) Combine pre-processing (reweighing) with in-processing or post-processing methods and validate across multiple fairness metrics; (2) Remove or carefully transform proxy features correlated with race; (3) Define acceptable fairness thresholds with domain experts and stakeholders; (4) Implement continuous monitoring, logging, and retraining triggers when data drift is detected; (5) Ensure human oversight, clear redress mechanisms, and public documentation of model limitations and governance.\n",
    "\n",
    "Ultimately, fairness is contextual — legal and ethical frameworks should guide threshold choice, and deployment must include human review and accessible means for affected individuals to contest decisions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b1761f",
   "metadata": {},
   "source": [
    "\n",
    "## Part 4: Ethical Reflection (5%)\n",
    "\n",
    "**Reflection:**\n",
    "\n",
    "In my CropSight project, ethical risks include biased training data, model mistakes leading to poor farmer decisions, and privacy concerns. To address these, I will: collect diverse field data across regions and seasons; document data provenance and consent; run fairness audits across subgroups (regions, farm sizes); provide explainable outputs and uncertainty estimates; keep human-in-the-loop for high-stakes recommendations; obfuscate precise coordinates in public views; and implement continuous monitoring and retraining to handle drift. These steps help ensure CropSight is accountable, equitable, and useful.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3d4186",
   "metadata": {},
   "source": [
    "\n",
    "## Bonus Task — 1-Page Policy Proposal: Ethical AI in Healthcare\n",
    "\n",
    "**Title:** Ethical AI Guidelines for Healthcare Deployments\n",
    "\n",
    "**Purpose:** Ensure AI systems in healthcare respect patient rights, are clinically safe, equitable, and transparent.\n",
    "\n",
    "**Key points:**\n",
    "- Patient consent & data governance: informed consent, data minimization, de-identification.\n",
    "- Bias mitigation & fairness: audit datasets, use fairness metrics, clinical validation across subgroups.\n",
    "- Explainability & clinical integration: provide rationales and confidence, AI as decision support.\n",
    "- Safety & validation: pilot studies, safety incident tracking, rollback criteria.\n",
    "- Privacy & security: encryption, RBAC, compliance with HIPAA/GDPR/local law.\n",
    "- Accountability & governance: AI ethics officer, model cards, public summaries.\n",
    "- Redress: mechanisms for patients to query and opt-out.\n",
    "\n",
    "(Use this as a 1-page PDF submission for the bonus.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcd56ac",
   "metadata": {},
   "source": [
    "\n",
    "## Submission Structure & Notes\n",
    "\n",
    "Suggested repo layout:\n",
    "\n",
    "```\n",
    "AI_Ethics_Assignment/\n",
    "├── report.pdf                   # Written answers + case studies + reflections\n",
    "├── notebook/\n",
    "│   └── compas_fairness_audit.ipynb\n",
    "├── data/\n",
    "│   └── compas.csv (if allowed)\n",
    "├── visuals/\n",
    "│   └── fpr_fnr_by_group.png\n",
    "└── policy_bonus.pdf             # 1-page policy proposal\n",
    "```\n",
    "\n",
    "**Notes:**\n",
    "- The code cells related to AIF360 require the library to be installed and may need specific versioning (AIF360 0.4.0 recommended).\n",
    "- Run the install cell at the top of Part 3 in a compatible environment (local conda or cloud VM) if the environment cannot install AIF360.\n",
    "\n",
    "Good luck — adapt the notebook to include your group's names and peer review notes before exporting to PDF for submission.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
